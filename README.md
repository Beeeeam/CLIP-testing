# CLIP-testing
We investigate the performance of the CLIP(Contrastive Languageâ€“Image Pre-training) model in image-text matching tasks, highlighting its robust capabilities in multimodal learning through contrastive learning techniques. The dataset used consists of 8000 image-text pairs.
